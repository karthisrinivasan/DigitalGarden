[[Gert Cauwenberghs]]

- Spiking neurons?
	- ReLU, sigmoid, tanh as of now
- On-chip learning?
- Chip-in-the-loop training is done one layer at a time
	- Needs software layer-level output as reference
	- Comparable to software model with 4bit weight precision
- Scaling down with process
	- 130nm to 7nm: 2-3x EDP improvement
- Power consumption reduction
	- ![[Pasted image 20210911114244.png]]
- RRAM device scaling
	- Lower forming voltage -> higher density, lower parasitics
- Other architectures besides voltage integrating neurons